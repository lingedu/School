{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Note that we normalize the training data by dividing the each pixel value with 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "x_train, t_train, x_test, t_test = mnist.load()\n",
    "xtrain=tensor(x_train/255,dtype=torch.float)\n",
    "ttrain=tensor(t_train,dtype=torch.int64)\n",
    "xtest=tensor(x_test/255,dtype=torch.float)\n",
    "ttest=tensor(t_test,dtype=torch.int64)\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Model\n",
    "We use CNN (convolution neural network) to predict the data.\n",
    "* Conv1: We use **Conv2d(1,6,5)** to build our first convolution layer. Since our digit image is not colorful, it only have one channel. We set the first parameter to one as the inital channel of given image. We set our output channel to six, and set our kernel size to five (i.e., each filter kernel is 5*5)\n",
    "\n",
    "* Conv2: we call **Conv2d(6,16,5)** which accept six-channel input to builf second convolution layer. We set the out channel to sixteen and the kernal size is five (5*5 kernel).\n",
    "\n",
    "* MaxPooling: After each convolution layer, we will call **MaxPool2d(2,2)** to do the max pooling. We set the size of the window (2,2) to take a max over.\n",
    "\n",
    " \n",
    "\n",
    "In the Net.forward() we set our procedure.\n",
    "* We first apply *conv1*, call the relu (rectified linear unit) funtion to filter the unnecessary features, and then apply max pooling.\n",
    "* We then apply *conv2*, call the relu function, and then apply max pooling.\n",
    "* We transform the dataset to have 256 columns which can fit in the the fisrt linear layer *fc1*, and then apply *tanh* \n",
    "* We fit the data to second linear layer *fc2* and then apply *softmax* along with dimension 1 (softmax between different classes)\n",
    "\n",
    "Note that we set the first linear layer transform data with 256 features. Since sfter we fit the data into conv1, max pooling, conv2, and max pooling again, the data shape became (16,4,4) where 16 denotes the channel number and other two denote the width and height. So after we flatten the data, the data size became 256(4*4*16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(256, 120)#(28-4)/2=12,(12-4)/2=4(size=4*4), there are 16 filters\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,256)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x=F.softmax(self.fc2(x),dim=1)\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training \n",
    "* CrossEntropyLoss: cross entropy loss in pytorch already encoded softmax. so we mark out the softmax function in our model (above cell).\n",
    "\n",
    "* optimizer: We use the most popular optimer Adam, a methed for a method for stochastic Optimization. We also try the SGD(Stochastic gradient descent) with momentun 0.9, but the result is not good, only achive 0.7-0.8 accuracy.\n",
    "\n",
    "* We run 20 epoch, in each epoch:\n",
    "    * **optimizer.zero_grad** is to set gradient to zero to avoid mixing the result with previous epoch.\n",
    "    * we fit the training data (xtrain) to our model, and have the output y\n",
    "    * we compute the cross entropy loss with our output y and the training labels\n",
    "    * we call **loss.backward** to run the backpogation to and compute the error derivative for model parameters\n",
    "    * we call **optimizer.step** to update the model parameters with the \n",
    "    \n",
    "* We can see the loss decreases with increasing epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss: 2.3072292804718018\n",
      "epoch:  0 loss: 2.4611213207244873\n",
      "epoch:  0 loss: 2.332186460494995\n",
      "epoch:  0 loss: 2.338225841522217\n",
      "epoch:  0 loss: 2.261850357055664\n",
      "epoch:  0 loss: 2.4608004093170166\n",
      "epoch:  0 loss: 2.461057424545288\n",
      "epoch:  0 loss: 2.461148262023926\n",
      "epoch:  0 loss: 2.4611024856567383\n",
      "epoch:  0 loss: 2.4611501693725586\n",
      "epoch:  0 loss: 2.4611496925354004\n",
      "epoch:  0 loss: 1.461152195930481\n",
      "epoch:  0 loss: 2.4611501693725586\n",
      "epoch:  0 loss: 2.4611501693725586\n",
      "epoch:  0 loss: 2.4611501693725586\n",
      "epoch:  0 loss: 2.4611501693725586\n",
      "epoch:  0 loss: 2.4611501693725586\n",
      "epoch:  0 loss: 2.4611501693725586\n",
      "epoch:  0 loss: 2.4611501693725586\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr = 0.01)\n",
    "\n",
    "start=time()\n",
    "for epoch in range(1):\n",
    "    for i,x in enumerate(xtrain):\n",
    "        optimizer.zero_grad()\n",
    "        y = net(x.view(-1,1,28,28))\n",
    "        loss = criterion(y, tensor([ttrain[i]]))\n",
    "        \n",
    "        if i%2000==0: print('epoch: ', epoch, 'loss:', loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ypred=net(xtest.view(-1,1,28,28))\n",
    "\n",
    "print(ypred.max(1)[1])\n",
    "print(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp=ttest.eq(ypred.max(1)[1])\n",
    "true=len([x for x in cmp if x==1])\n",
    "false=len([x for x in cmp if x==0])\n",
    "total=true+false\n",
    "print(true/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
