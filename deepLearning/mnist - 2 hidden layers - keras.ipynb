{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist\n",
    "x_train, t_train, x_test, t_test = mnist.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data and make one-hot array for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) 5\n",
      "(10000, 784) 7\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "xtrain=x_train/255\n",
    "ttrain=to_categorical(t_train)\n",
    "\n",
    "xtest=x_test/255\n",
    "ttest=to_categorical(t_test)\n",
    "\n",
    "print(xtrain.shape, ttrain[0].argmax())\n",
    "print(xtest.shape, ttest[0].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Keras and set the model\n",
    "First hidden layer:\n",
    "* We set the first hidden layer by calling Dense function, note that, we should set **input_dim** to denote the input dimension. In this case, each digit has 784 pixels, so the input dimention is 784.\n",
    "*  We 'relu'(rectifier) as our activation function in the first hiden layer\n",
    "\n",
    "Second hidden layer:\n",
    "* We call Dropout(0.5) to drop out 50% imput for the second hidden layer. Dropout is a regularization technique which help to reduce overfitting.\n",
    "* We set the second hidden layer with 30 neurons, and applied 'relu' activation.\n",
    "\n",
    "Output layer:\n",
    "* We set the output layer with 10 neurons (digit 0-9), and apply softmax on the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Dense(80, input_dim=784),#input_shape=(784,)),\n",
    "    Activation(tf.nn.relu),\n",
    "    Dropout(0.5),\n",
    "    Dense(30),\n",
    "    Activation(tf.nn.relu),\n",
    "    Dense(10),\n",
    "    Activation(tf.nn.softmax)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training \n",
    "* __SGD__:\n",
    "Before compile the model, we we call SGD (Stochastic gradient descent) as our optimizer. \n",
    "    * *lr* is the leaning rate\n",
    "    * *decay* is learning rate decay (over each epoch)\n",
    "    * we set *momentum* 0.9 and set *nesterov* as true.\n",
    "    \n",
    "    Note: We set _momentum_ and _nesterov_ since SGD with momentum can overcome local optimum/push torward to global optimum, and Nesterov momentum result in good acceleration.\n",
    "\n",
    "\n",
    "* __Model.compile__ is to compile the model:\n",
    "    * We assign to *categorial_crossentropy*, which is adapted in multi-calss single-label classification. \n",
    "    * We set our optimizer to our sgd optomizer we create above. \n",
    "    * And we assign 'accuracy' to *metrics* to show the accuracy for each epoch.\n",
    "\n",
    "\n",
    "* __Model.fit__: After compile our model, we now can fit our training data(xtrain) and labels(ttrain) to our model, and we set the batch size 128 and run 3 epochs total.\n",
    "\n",
    "# result\n",
    "We can the time and accuracy of each epoch. As the result shows, in the epoch 3, the accuracy already get 0.9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.7687 - acc: 0.7588\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.3851 - acc: 0.8858\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.3182 - acc: 0.9041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8eb153f28>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ttrain,\n",
    "          epochs=3,\n",
    "          batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "We use **model.evaluate** to evaluate the model on testing data(xtest) and labels(ttest). The print out the score, the first item is the loss, which is 0.18, and the second item is accuracy which is 0.94. We spend 6 second to train the model with at least 90% accuracy, which is a good performance. Note that by tuning the parameters like learning rate, batch size or epochs, it is possible to accelerate the training process and get higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 15us/step\n",
      "[0.18491371276378632, 0.94159999999999999]\n",
      "acc: 94.16\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(xtest, ttest, batch_size=128)\n",
    "print(score)\n",
    "print(\"%s: %.2f\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other way to add layer in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Dense(64, activation='relu', input_dim=20))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(10, activation=tf.nn.softmax))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
