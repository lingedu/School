{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "x_train, t_train, x_test, t_test = mnist.load()\n",
    "\n",
    "xtrain=tensor(x_train/255,dtype=torch.float)\n",
    "ttrain=tensor(t_train,dtype=torch.int64)\n",
    "xtest=tensor(x_test/255,dtype=torch.float)\n",
    "ttest=tensor(t_test,dtype=torch.int64)\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the model\n",
    "We define a class Net which inherent from **nn.Module** \n",
    "\n",
    "Intial model:\n",
    "\n",
    "**nn.Linear**: apply a linear transformation to the incoming data\n",
    "1. We set the first layer *ly1* as a linear transformation from 784 neurons (784 pixels) to 80 neurons.\n",
    "2. We set the second layer *ly2* as a linear transformation from 80 neurons to 30 neurons. \n",
    "3. We set the third layer *ly3* as a linear transformation from 30 neurons to 10 neurons (digit 0-9).\n",
    "\n",
    "**Note:** We set *ly1_drop* to drop out 50% of the output of *ly1*, and *ly2_drop* to drop out 50% of the output of *ly2*. But since the performance is not good with drop out here, we mark it out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.ly1 = nn.Linear(784,80)\n",
    "        #self.ly1_drop = torch.nn.Dropout(0.5) #we try drop out but not work well here\n",
    "        self.ly2 = nn.Linear(80,30)\n",
    "        #self.ly2_drop = torch.nn.Dropout(0.5)\n",
    "        self.ly3 = nn.Linear(30,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.ly1(x))\n",
    "        #x = self.ly1_drop(torch.relu(self.ly1(x))) #we try drop out but not work well\n",
    "        x = torch.relu(self.ly2(x))\n",
    "        #x = self.ly2_drop(torch.relu(self.ly2(x)))\n",
    "        x=self.ly3(x)\n",
    "        #x=torch.softmax(self.ly3(x),dim=1) #cross entropy already encode th softmax\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training \n",
    "* CrossEntropyLoss: cross entropy loss in pytorch already encoded softmax. so we mark out the softmax function in our model (above cell).\n",
    "\n",
    "* optimizer: We first try the SGD(Stochastic gradient descent) with momentum. The result is 0.967, and takes 2.1 seconds. Then we use optimer Adam, an other popular methed for a method for stochastic optimization. We had accuray 0.97, and time 2.7 seconds.\n",
    "\n",
    "* We run 5 epochs, and divide the data into batches.\n",
    "    * We extract batch *x* from training data and batch *t* labels.\n",
    "    * **optimizer.zero_grad** is to set the gradients to zero. It can avoid mixing the result with previous epoch.\n",
    "    * we fit the training data (batch *x*) to our model, and have the output y\n",
    "    * we compute the cross entropy loss with our output y and the training labels (batch *t*)\n",
    "    * we call **loss.backward** to run the back propagation to compute the error derivatives for model parameters\n",
    "    * we call **optimizer.step** to update the model parameters with the result from back propagation.\n",
    "    \n",
    "**Note that one epoch can achieve 0.93 accuracy and only take 0.34 seconds.**\n",
    "\n",
    "Note that we can change the batch sizes. Larger batch size may run faster but not necessary have better perofrmance. In some case, batch size too large consumes too much memory and slow down the speed. So users can choose different size depend on the data size and computer load.\n",
    "\n",
    "The red mark out is the experiment without using batches. The result is not too good, it takes more time but has lower accuracy even with 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14222663640975952\n",
      "time 2.082205057144165\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(net.parameters(),lr = 0.01)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\"\"\"\n",
    "for epoch in range(20):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        y = net(xtrain)\n",
    "        loss = criterion(y, ttrain)\n",
    "        print('epoch: ', epoch, 'loss:', loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print('done')\n",
    "\"\"\"\n",
    "\n",
    "stime=time()\n",
    "tsize=xtrain.shape[0] #Total size of the data.\n",
    "bsize=300 #Batch size of the data. We also try 30 or 2000.\n",
    "for epoch in range(5):\n",
    "    for i in range(int(tsize/bsize)):\n",
    "            x=xtrain[bsize*i:bsize*(i+1)]\n",
    "            t=ttrain[bsize*i:bsize*(i+1)]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y = net(x)\n",
    "            loss = criterion(y, t)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "print(loss.item())\n",
    "print('time',time()-stime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "We fit the testing data(xtest) into our mode, and print out the result and compare with the testing labels. And got the final accuracy 0.9074.\n",
    "\n",
    "Note that by tuning the parameters like learning rate, batch size or epochs, it is a chance to accelerate the training process or improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1,  ..., 4, 5, 6])\n",
      "tensor([7, 2, 1,  ..., 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "ypred=net(xtest)\n",
    "\n",
    "print(ypred.max(1)[1])\n",
    "print(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cmp=ttest.eq(ypred.max(1)[1])\n",
    "#true=len([x for x in cmp if x==1])\n",
    "#false=len([x for x in cmp if x==0])\n",
    "#total=true+false\n",
    "#print(true/total)\n",
    "\n",
    "float((ttest==ypred.max(1)[1]).sum())/len(ttest) #compute the accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
