{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.tensor as tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_train, t_train, x_test, t_test = mnist.load()\n",
    "\n",
    "xtrain=tensor(x_train/255,dtype=torch.float)\n",
    "ttrain=tensor(t_train,dtype=torch.int64)\n",
    "xtest=tensor(x_test/255,dtype=torch.float)\n",
    "ttest=tensor(t_test,dtype=torch.int64)\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the model\n",
    "We define a class Net which inherent from **nn.Module** \n",
    "\n",
    "Intial model:\n",
    "\n",
    "**nn.Linear**: apply a linear transformation to the incoming data\n",
    "1. We set the first layer *ly1* as a linear transformation from 784 neurons (784 pixels) to 80 neurons.\n",
    "2. We set the second layer *ly2* as a linear transformation from 80 neurons to 30 neurons. \n",
    "3. We set the third layer *ly3* as a linear transformation from 30 neurons to 10 neurons (digit 0-9).\n",
    "\n",
    "**Note:** We set *ly1_drop* to drop out 50% of the output of *ly1*, and *ly2_drop* to drop out 50% of the output of *ly2*. But since the performance is not good with drop out here, we mark it out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.ly1 = nn.Linear(784,80)\n",
    "        #self.ly1_drop = torch.nn.Dropout(0.5) #we try drop out but not work well here\n",
    "        self.ly2 = nn.Linear(80,30)\n",
    "        #self.ly2_drop = torch.nn.Dropout(0.5)\n",
    "        self.ly3 = nn.Linear(30,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.ly1(x))\n",
    "        #x = self.ly1_drop(torch.relu(self.ly1(x))) #we try drop out but not work well\n",
    "        x = torch.relu(self.ly2(x))\n",
    "        #x = self.ly2_drop(torch.relu(self.ly2(x)))\n",
    "        x=self.ly3(x)\n",
    "        #x=torch.softmax(self.ly3(x),dim=1) #cross entropy already encode th softmax\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training \n",
    "* CrossEntropyLoss: cross entropy loss in pytorch already encoded softmax. so we mark out the softmax function in our model (above cell).\n",
    "\n",
    "* optimizer: We use the most popular optimer Adam, a methed for a method for stochastic Optimization. We also try the SGD(Stochastic gradient descent) with momentun 0.9, but the result is not good, only achive 0.7-0.8 accuracy.\n",
    "\n",
    "* We run 20 epoch, in each epoch:\n",
    "    * **optimizer.zero_grad** is to set gradient to zero to avoid mixing the result with previous epoch.\n",
    "    * we fit the training data (xtrain) to our model, and have the output y\n",
    "    * we compute the cross entropy loss with our output y and the training labels\n",
    "    * we call **loss.backward** to run the backpogation to and compute the error derivative for model parameters\n",
    "    * we call **optimizer.step** to update the model parameters with the \n",
    "    \n",
    "* We can see the loss decreases with increasing epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss: 2.306840181350708\n",
      "epoch:  1 loss: 2.216726303100586\n",
      "epoch:  2 loss: 2.009770631790161\n",
      "epoch:  3 loss: 1.7456607818603516\n",
      "epoch:  4 loss: 1.4443638324737549\n",
      "epoch:  5 loss: 1.1498793363571167\n",
      "epoch:  6 loss: 0.9130261540412903\n",
      "epoch:  7 loss: 0.7470263242721558\n",
      "epoch:  8 loss: 0.6418042778968811\n",
      "epoch:  9 loss: 0.5765392780303955\n",
      "epoch:  10 loss: 0.5262182950973511\n",
      "epoch:  11 loss: 0.4958030581474304\n",
      "epoch:  12 loss: 0.4630966782569885\n",
      "epoch:  13 loss: 0.4482825994491577\n",
      "epoch:  14 loss: 0.4266660213470459\n",
      "epoch:  15 loss: 0.4154680073261261\n",
      "epoch:  16 loss: 0.4017115831375122\n",
      "epoch:  17 loss: 0.3927520215511322\n",
      "epoch:  18 loss: 0.38061603903770447\n",
      "epoch:  19 loss: 0.37106236815452576\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() # mean square error loss\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr = 0.01)\n",
    "for epoch in range(20):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        y = net(xtrain)\n",
    "        loss = criterion(y, ttrain)\n",
    "        print('epoch: ', epoch, 'loss:', loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "We fit the testing data(xtest) into our mode, and print out the result and compare with the testing labels. And got the final accuracy 0.9074.\n",
    "\n",
    "Note that by tuning the parameters like learning rate, batch size or epochs, it is a chance to accelerate the training process or improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1,  ..., 4, 5, 6])\n",
      "tensor([7, 2, 1,  ..., 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "ypred=net(xtest)\n",
    "\n",
    "print(ypred.max(1)[1])\n",
    "print(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9074"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cmp=ttest.eq(ypred.max(1)[1])\n",
    "#true=len([x for x in cmp if x==1])\n",
    "#false=len([x for x in cmp if x==0])\n",
    "#total=true+false\n",
    "#print(true/total)\n",
    "\n",
    "float((ttest==ypred.max(1)[1]).sum())/len(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
