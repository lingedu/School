{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (Convolution Neural Netwrok)\n",
    "CNN is a popular deep learning model in dealing with images. We implement CNN with keras here to compare the performance with the multi-layer model. We also implement CNN with pytorch in other file to make comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Note that we normalize the training data by dividing the each pixel value with 255. \n",
    "\n",
    "We also transform the data shape to (-1, 28, 28, 1), where -1 means automatically computed by the function according to other parameters. The width and height of the images after transformation are 28 and 28, respentively. Since the images is non-colorful, there only one chanel (1 denote the channel size). We then plot the first digit of the traning data, which is 5 as shown below.\n",
    "\n",
    "Besides, keras model require one-hot arrays for the labels. So we call **to_categorical** to transform the labels to one-hot arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) 5\n",
      "(10000, 784) 7\n",
      "(60000, 28, 28, 1) 5\n",
      "(10000, 28, 28, 1) 7\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "import mnist\n",
    "x_train, t_train, x_test, t_test = mnist.load()\n",
    "\n",
    "xtrain=x_train/255\n",
    "ttrain=to_categorical(t_train)\n",
    "\n",
    "xtest=x_test/255\n",
    "ttest=to_categorical(t_test)\n",
    "\n",
    "print(xtrain.shape, ttrain[0].argmax())\n",
    "print(xtest.shape, ttest[0].argmax())\n",
    "\n",
    "xtrain=np.asarray(tensor(xtrain).view(-1,28,28,1))\n",
    "xtest=np.asarray(tensor(xtest).view(-1,28,28,1))\n",
    "print(xtrain.shape, ttrain[0].argmax())\n",
    "print(xtest.shape, ttest[0].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Model\n",
    "We set our model by calling Sequential() in keras.models. \n",
    "\n",
    "* We first call convolution layer with 'relu' (rectified linear unit) function, and then apply max pooling.\n",
    "\n",
    "    * In the first convolution layer, we assign the six-channel output (6 filters). We set the input image shape (28, 28, 1) - width 28, heigh 28, and channel 1. We filter the image with a 5x5 window.  \n",
    "    * We use 'relu' as our activation function to filter the unnecessary features by set negative values to 0s.\n",
    "    * We set the size of the window (2,2) to take a max over.\n",
    "\n",
    "* We then call another convolution layer again.\n",
    "    * In the second convolution layer, we assign the sixteen-channel output (16 filters). We filter the image with a 5x5 window.  \n",
    "    * We use 'relu' as our activation function.\n",
    "    * We set the size of the window (2,2) to take a max over.\n",
    "\n",
    "\n",
    "* We flatten the data\n",
    "* Dense() is return a regular densely-connected neural network layer. We build a layer with 70 neurons and set 'relu' as the activation function. We will fit the faltten data from above step into the 70-dense layer.\n",
    "* We build the output layer with 10 neurons(represent 0-9 digits respectively) and apply softmax the output array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    Conv2D(6, (5,5), input_shape=(28,28,1),activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    #Dropout(0.5),\n",
    "    \n",
    "    Conv2D(16, (5,5), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    #Dropout(0.5),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    #Dense(256, activation='relu'),#input_shape=(784,)),\n",
    "    #Dropout(0.5),\n",
    "    \n",
    "    Dense(70,activation='relu'),\n",
    "    #Dropout(0.5),\n",
    "    \n",
    "    Dense(10,activation=tf.nn.softmax),\n",
    "    #Dense(10,activation='linear'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training \n",
    "* Compile the model\n",
    "    * categorical_crossentropy: often applied in multi-calss one label classification.\n",
    "    * optimizer: We apply both SGD (Stochastic gradient descent) and Adam (another popular methed for a method for stochastic optimization). The performance are both good.\n",
    "    * We set accuracy to the metrics so that it will report accuracy in each epoch.\n",
    "\n",
    "* Training model\n",
    "\n",
    "We fit the training data and lebels to our model. We run one epoch and set the batch size to 300. It means that we train the data set with batches and each batch has 300 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 0.5697 - acc: 0.8467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe69375df60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(xtrain, ttrain,\n",
    "          epochs=1,\n",
    "          batch_size=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "**Model.evaluate**: returns the loss value and metrics values (we set 'accuracy' above) for the model in test mode. We can see the loss is 0.13 and accuracy is 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 167us/step\n",
      "[0.1493694712638855, 0.95640000000000003]\n",
      "acc: 95.64\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(xtest, ttest, batch_size=128)\n",
    "print(score)\n",
    "print(\"%s: %.2f\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to pytorch\n",
    "We implement CNN pytorch and kearas. We construct the same model, and run both model in one epoch. Model with Keras runs 16s and has accuracy 0.95 while model with Pytorch runs 7s and has accuracy 0.97. Pytorch perform faster and better.\n",
    "\n",
    "# Compare the multi-layer network\n",
    "We implement multi-layer network in keras in other file. It runs 8 epochs with 16 seconds but only achieve accuracy 0.93. We can see CNN perform better(16s; accuracy 0.95; one epoch), but each epoch takes more time than multi-layer network.\n",
    "\n",
    "We guess the difference is because CNN involves complicated computaion which lower down the speed but can achieve high accuracy while only run the data once.\n",
    "\n",
    "In conclude, since the images in MNIST is not too complicated, multi-layer network can have good efficiency and CNN has better performance. But in the case of dealing complicated images, CNN should be a better way to adopt for its sophiscated computation,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
